---
title: "BNLL CEWL Data Wrangling"
author: "Savannah Weaver"
output: pdf_document
toc: TRUE
---


# Packages

```{r setup, include=FALSE}
`%nin%` = Negate(`%in%`)
if (!require("tidyverse")) install.packages("tidyverse")
library("tidyverse") # workflow and plots
```


# Background and Goals

This CEWL (cutaneous evaporative water loss) data was measured in 3-5 technical replicates on the mid-dorsum of Blunt-nosed Leopard Lizards (*Gambelia sila*) between April - July 2021. In this R script, I check the distribution of replicates, omit outliers, and average remaining replicates. The final values will be more precise and accurate estimates of the true CEWL for each lizard, and those values will be used in the analyses R script file. Please refer to **doi:** for the published scientific paper and full details.



# Load Data

1. Compile a list of the filenames I need to read-in.

```{r}
# make a list of file names of all data to load in
filenames <- list.files(path = "data/CEWL")
```

2. Make a function that will read in the data from each csv, name and organize the data correctly. 

```{r}
read_CEWL_file <- function(filename) {
  
  dat <- read.csv(file.path("data/CEWL", filename),
                  na.strings=c("","NA"),
                # each csv has headers
                header = TRUE
                ) %>%
    # select only the relevant values
    dplyr::select(date = Date, 
                  time = Time, 
                  status = Status,
                  ID_rep_no = Comments,
                  CEWL_g_m2h = 'TEWL..g..m2h..', 
                  msmt_temp_C = 'AmbT..C.', 
                  msmt_RH_percent = 'AmbRH....'
                  ) %>%
    # extract individual_ID and replicate number
    dplyr::mutate(ID_rep_no = as.character(ID_rep_no),
                  ID_len = as.factor(nchar(ID_rep_no)),
                  
                  individual_ID = as.factor(case_when(
                    ID_len == 7 ~ as.character(paste(substr(ID_rep_no, 1, 1),
                                             substr(ID_rep_no, 3, 5),
                                             sep = "")),
                    ID_len == 6 & substr(ID_rep_no, 1, 1) == "W" 
                        ~ as.character(substr(ID_rep_no, 1, 4)),
                    ID_len == 6 & substr(ID_rep_no, 1, 1) %in% c("M", "F") 
                        ~ as.character(paste(substr(ID_rep_no, 1, 1),
                                             substr(ID_rep_no, 3, 4),
                                             sep = "")),
                    ID_len == 5 ~ as.character(substr(ID_rep_no, 1, 3))
                    )),
                  
                  # works
                  replicate_no = as.factor(case_when(
                    ID_len == 7 ~ as.character(substr(ID_rep_no, 7, 7)),
                    ID_len == 6 ~ as.character(substr(ID_rep_no, 6, 6)),
                    ID_len == 5 ~ as.character(substr(ID_rep_no, 5, 5))
                    )))
  
  # return the dataframe for that single csv file
  dat
}
```

3. Apply the function I made to all of the filenames I compiled, then put all of those dataframes into one dataframe. This will print warnings saying that header and col.names are different lengths, because the data has extra notes cols that we read-in, but get rid of. Additionally, filter out failed measurements and properly format data classes.

```{r}
# apply function to get data from all csvs
all_CEWL_data <- lapply(filenames, read_CEWL_file) %>%
  # paste all data files together into one df by row
  reduce(rbind) %>%
  # filter out failed measurements
  dplyr::filter(status == "Normal") %>%
  # correctly format data classes
  mutate(date = as.Date(date, format = "%m/%d/%y"),
         time = as.POSIXct(time, format = "%H:%M"),
         status = as.factor(status)
         )

summary(all_CEWL_data)
```




# Check Data

Each lizard measured on each date should have 3-5 technical replicates, and those measurements should have been taken around the same time. 

```{r}
all_CEWL_data %>%
                group_by(individual_ID, date) %>%
                summarise(n = n(),
                          time_range = max(time) - min(time)) %>% 
                arrange(n)
```

The number of measurements taken is good! Almost always 3 or 5, with two lizards who only got 4 measurements, which is fine. But, M01 on April 23 and M03a on July 14 have abnormal time ranges of 43140 seconds (almost 12h), so we need to check that data.

```{r}
all_CEWL_data %>% dplyr::filter(individual_ID %in% c("M01", "M03A"))
```

Aha, it seems the problem is that the time isn't perfectly formatted, so 1 pm is coded as 1 am --> the measurements in question went across hours of 12 noon to 1 pm, so when reformatted, it seems like 1 am to 12 pm. It's fine as-is, and nothing is amiss with the data.





# Replicates

## Assess Variation

We want the Coefficient of Variation (CV) among our technical replicates to be small. We need to calculate it to identify whether there may be outliers.

```{r asses variation}
CVs <- all_CEWL_data %>%
  group_by(individual_ID, date) %>%
  summarise(mean = mean(CEWL_g_m2h),
            SD = sd(CEWL_g_m2h),
            CV = (SD/mean) *100,
            min = min(CEWL_g_m2h),
            max = max(CEWL_g_m2h),
            range = max - min
            )
summary(CVs)
hist(CVs$CV)
hist(CVs$range) 
```

We expect CV for technical replicates to be < 10-15%, so we must determine whether the CVs > 15% are due to outlier replicates. The range should also generally be within 5 units for these measurements. :(


## Find Outliers

First, create a function to look at the replicates for each individual on each day. For each iteration, I will make a boxplot and extract any outliers, compiling a dataframe of outliers that I want to exclude from the final dataset. By printing the boxplots and compiling a dataframe of outliers, I can check the data against the plots to ensure confidence in the outliers quantified.

```{r function to find outliers}
# write function to find outliers for each individual on each date
find_outliers <- function(df) {
  
  # initiate dataframe to compile info and list to compile plots
  outliers <- data.frame()
  #boxplots <- list()

  # initiate a for loop to go through every who in df
  for(indiv_ch in unique(df$individual_ID)) {
    
    # select data for only the individual of interest
    df_sub <- df %>%
      dplyr::filter(individual_ID == (indiv_ch))
    
    # make a boxplot
    df_sub %>%
      ggplot(.) +
      geom_boxplot(aes(x = as.factor(date),
                       y = CEWL_g_m2h,
                       fill = as.factor(date))) +
      ggtitle(paste("Individual", indiv_ch)) +
      theme_classic() -> plot
    
    # print/save
    print(plot)
    #boxplots[[indiv_ch]] <- plot
    
    # extract outliers
    outs <- df_sub %>%
      group_by(individual_ID, date) %>%
      summarise(outs = boxplot.stats(CEWL_g_m2h)$out)
    
    # add to running dataframe of outliers
    outliers <- outliers %>%
      rbind(outs)
  }
  #return(boxplots)
  return(outliers)
}
```


Now apply the function to the data:

```{r show outliers, fig.show = "hold", out.width = "50%"}
par(mfrow = c(71, 2))
outliers_found <- find_outliers(all_CEWL_data)
outliers_found
par(mfrow = c(1, 1))
```

Based on the plots, the dataframe of outliers I compiled is correct. (yay!)


## Remove Outliers

Now I will create a secondary version of the same function, but instead of compiling outliers, I will omit them from the dataset.

```{r function to remove outliers}
# write function to find and exclude outliers
omit_outliers <- function(df) {
  
  # initiate dataframe to compile info and list to compile plots
  cleaned <- data.frame()

  # initiate a for loop to go through every who in df
  for(indiv_ch in unique(df$individual_ID)) {
    
    # select data for only the individual of interest
    df_sub <- df %>%
      dplyr::filter(individual_ID == (indiv_ch))
    
    # extract outliers
    outs <- df_sub %>%
      group_by(individual_ID, date) %>%
      summarise(outs = boxplot.stats(CEWL_g_m2h)$out)
    
    # filter outliers from data subset for this individual
    filtered <- df_sub %>%
      dplyr::filter(CEWL_g_m2h %nin% outs$outs)
    
    # add to running dataframe of cleaned data
    cleaned <- cleaned %>%
      rbind(filtered)
  }
  return(cleaned)
}
```


Apply function to data and check that the new data subsets still contain the right amount of data:

```{r omit outliers, message = FALSE}
outliers_omitted <- omit_outliers(all_CEWL_data)
nrow(all_CEWL_data) == nrow(outliers_omitted) + nrow(outliers_found)
```




## Re-Assess Variation

```{r re-check variation}
new_CVs <- outliers_omitted %>%
  group_by(individual_ID, date) %>%
  summarise(mean = mean(CEWL_g_m2h),
            SD = sd(CEWL_g_m2h),
            CV = (SD/mean) *100,
            min = min(CEWL_g_m2h),
            max = max(CEWL_g_m2h),
            range = max - min)
summary(new_CVs)
hist(new_CVs$CV)
hist(CVs$CV)
hist(new_CVs$range) 
hist(CVs$range) 
```


This definitely improved things, but unfortunately, CVs are still skewed to the right. I think the replicate groups with only 3 replicates are harder to find outliers in, so let's compute pairwise CVs and see if we can still minimze the larger range/CV values.



## Find "Outliers"

Determine which replicates lead to an increased CV...

```{r find super different values}
# which individuals have how many reps
n_reps <- outliers_omitted %>%
  group_by(individual_ID, date) %>%
  summarise(n = (n()))

# prep dfs for computing CVs
oo_small_345 <- outliers_omitted %>%
  dplyr::select(individual_ID, date, CEWL_g_m2h, replicate_no)

oo_small_45 <- oo_small_345 %>%
  left_join(n_reps, by = c('individual_ID', 'date')) %>%
  dplyr::filter(n %in% c(4, 5))

oo_small_5 <- oo_small_45 %>%
  dplyr::filter(n == 5)

# test excluding different replicates
CV_excl1 <- oo_small_345 %>%
    dplyr::filter(replicate_no != 1) %>%
    group_by(individual_ID, date) %>%
    summarise(mean = mean(CEWL_g_m2h),
              SD = sd(CEWL_g_m2h),
              CV = (SD/mean) *100) %>%
    mutate(rep_excluded = "1") 
CV_excl2 <- oo_small_345 %>% 
    dplyr::filter(replicate_no != 2) %>%
    group_by(individual_ID, date) %>%
    summarise(mean = mean(CEWL_g_m2h),
              SD = sd(CEWL_g_m2h),
              CV = (SD/mean) *100) %>%
    mutate(rep_excluded = "2") 
CV_excl3 <- oo_small_345 %>% 
    dplyr::filter(replicate_no != 3) %>%
    group_by(individual_ID, date) %>%
    summarise(mean = mean(CEWL_g_m2h),
              SD = sd(CEWL_g_m2h),
              CV = (SD/mean) *100) %>%
    mutate(rep_excluded = "3") 
CV_excl4 <- oo_small_45 %>% 
    dplyr::filter(replicate_no != 4) %>%
    group_by(individual_ID, date) %>%
    summarise(mean = mean(CEWL_g_m2h),
              SD = sd(CEWL_g_m2h),
              CV = (SD/mean) *100) %>%
    mutate(rep_excluded = "4") 
CV_excl5 <- oo_small_5 %>%
    dplyr::filter(replicate_no != 5) %>%
    group_by(individual_ID, date) %>%
    summarise(mean = mean(CEWL_g_m2h),
              SD = sd(CEWL_g_m2h),
              CV = (SD/mean) *100) %>%
    mutate(rep_excluded = "5") 
  

# figure out what replicate inflates CV (and range)
compare <- oo_small_345 %>%
    # first compute CV again with ALL replicates
    group_by(individual_ID, date) %>%
    summarise(mean = mean(CEWL_g_m2h),
              SD = sd(CEWL_g_m2h),
              CV = (SD/mean) *100) %>%
    mutate(rep_excluded = "none") %>%
    # attach other CVs with sub-setted rep numbers
    rbind(CV_excl1) %>%
    rbind(CV_excl2) %>%
    rbind(CV_excl3) %>%
    rbind(CV_excl4) %>%
    rbind(CV_excl5) %>%
    # compare which group of reps gives the lowest CV for each individual
    group_by(individual_ID, date) %>%
    dplyr::mutate(none_CV = case_when(rep_excluded == "none" ~ CV,
                                      rep_excluded != "none" ~ NA_real_),
                  min_CV = min(CV),
                  none_vs_min = none_CV - min_CV,
                  rep_excluded = as.factor(rep_excluded))
summary(compare)
hist(compare$none_vs_min, breaks = 100)
```


## Remove Outliers

```{r remove remaining outliers}
# take the "none" removed rep avg when none_vs_min <5
no_cleaning_needed <- compare %>%
  dplyr::filter(none_vs_min <= 5) %>%
  dplyr::select(individual_ID, date, rep_excluded)

# get rest
needs_cleaning_IDs <- compare %>%
  dplyr::filter(none_vs_min > 5) %>%
  dplyr::mutate(keep = "KEEP") %>%
  dplyr::select(individual_ID, date, keep)

# check number of data obs
test_n <- outliers_omitted %>% 
  group_by(individual_ID, date) %>%
  summarise(n = n())
nrow(test_n) == (nrow(no_cleaning_needed) + nrow(needs_cleaning_IDs))

# designate what to remove / not
how_cleaning <- compare %>%
  left_join(needs_cleaning_IDs) %>%
  dplyr::filter(keep == "KEEP") %>%
  dplyr::filter(CV == min_CV) %>%
  dplyr::select(individual_ID, date, rep_excluded) %>%
  rbind(no_cleaning_needed)

# check
nrow(test_n) == (nrow(how_cleaning))

# remove!! :D
cleaned_using_CVs <- outliers_omitted %>%
  left_join(how_cleaning, by = c("individual_ID", "date")) %>%
  dplyr::filter(replicate_no != as.character(rep_excluded))

# check that we improved things
check <- cleaned_using_CVs %>%
  group_by(individual_ID, date) %>%
  summarise(CEWL_g_m2h_mean = mean(CEWL_g_m2h),
            rep_SD = sd(CEWL_g_m2h),
            rep_CV = (rep_SD/CEWL_g_m2h_mean) *100,
            rep_min = min(CEWL_g_m2h),
            rep_max = max(CEWL_g_m2h),
            rep_range = rep_max - rep_min
            )
hist(check$rep_CV)
hist(new_CVs$CV)
hist(check$rep_range) 
hist(new_CVs$range)
```

Yes, we decreased the CVs and ranges even more, so I think worth the data scrubbing.


## Average Replicates (outliers removed)

```{r get replicate means}
CEWL_final <- cleaned_using_CVs %>%
  group_by(date, individual_ID) %>%
  summarise(CEWL_g_m2h = mean(CEWL_g_m2h),
            msmt_temp_C = mean(msmt_temp_C),
            msmt_RH_percent = mean(msmt_RH_percent))
head(CEWL_final)
```


# Final Synthesis

## Re-Check Data

Check that we still have data for every individual.

I can check this by comparing original individual IDs to the individual IDs in our final dataset, then selecting/printing the IDs used that are in one but not the other.

```{r re-check individual IDs}
unique(CEWL_final$individual_ID) %in% unique(all_CEWL_data$individual_ID)
unique(all_CEWL_data$individual_ID) %in% unique(CEWL_final$individual_ID)
```

All is as expected. :)

Check how many observations were used to calculate mean CEWL for each individual on each date:

```{r re-check n obs}
cleaned_using_CVs %>%
  group_by(individual_ID, date) %>%
  summarise(n = n()) %>% 
  arrange(n)
```


Between 2-5. So, We were able to delete a point if it was off from the other two in a group of 3.



## Export

Save the cleaned data for models and figures.

```{r save clean data}
write.csv(CEWL_final, "./data/CEWL_dat_all_clean.csv")
```





## Reporting

We omitted a total of 105 measurements from our CEWL dataset (465 - 351): 1 replicate was removed for most individuals. We used the boxplot.stats function in R to extract outliers from each set of technical replicates, and 24 points were removed this way. The remaining 81 removed replicates were taken out because they increased the CV for their replicate group by >5% compared to the CV without that point. After data cleaning, every individual still had 2-5 technical replicates for each of their measurement dates. The distribution of coefficient of variation values was much better after both data cleaning steps than before.


